n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
n_heads: 2                      # (int) The number of attention heads for multi-head attention layer.
hidden_size: 64                 # (int) The number of features in the hidden state.
inner_size: 256                 # (int) The inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.5        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.5          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.
layer_norm_eps: 1e-12           # (float) A value added to the denominator for numerical stability. 
initializer_range: 0.02         # (float) The standard deviation for normal initialization.
item_attribute: 'class'         # (str) The item features used as attributes for pre-training.
mask_ratio: 0.2                 # (float) The probability for a item replaced by MASK token.
aap_weight: 1.0                 # (float) The weight for Associated Attribute Prediction loss.
mip_weight: 0.2                 # (float) The weight for Masked Item Prediction loss.
map_weight: 1.0                 # (float) The weight for Masked Attribute Prediction loss.
sp_weight: 0.5                  # (float) The weight for Segment Prediction loss.
train_stage: 'pretrain'         # (str) The training stage. Range in ['pretrain', 'finetune'].
pretrain_epochs: 500            # (int) The epochs of pre-training.
save_step: 10                   # (int) Save pre-trained model every pre-training epochs.
pre_model_path: ''              # (str) The path of pretrained model.
loss_type: 'CE'                 # (str) The type of loss function. Range in ['BPR', 'CE'].