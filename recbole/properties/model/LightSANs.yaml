# k_interests (int): The number of latent interests dimension in item-to-interest aggregation. 
# k_interests equals to 0.1 * MAX_ITEM_LIST_LENGTH is suggested to ensure the compression effect.
k_interests: 5
n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
n_heads: 2                      # (int) The number of attention heads for multi-head attention layer.
hidden_size: 64                 # (int) The input and output hidden size.
inner_size: 256                 # (int) The imensionality in feed-forward layer.
hidden_dropout_prob: 0.5        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.5          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.
layer_norm_eps: 1e-12           # (float) The value added to the denominator for numerical stability.
initializer_range: 0.02         # (float) The range of weights initialization.
loss_type: 'CE'                 # (str) The type of loss function.